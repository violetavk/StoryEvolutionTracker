What I have so far:
- HTML is parsed
- word frequencies are calculated
- words are stemmed 
- tf-idf's calculated
- top n "important" words are found

Need to recalculate tf-idf's, because now I need to choose an approach:
- use original words to calculate tf-idf's, with words like "apartments", and "apartment" remaining (as-is)
- or, for use the stemmed words to calculate tf-idf's
- or, re-calculate tf-idf's for each stemmed word, where the tf-idf value is average of all of its original words
- or, for each original word, stem it, get it's other forms, get tf-idf values of those, and take average (good that it keeps the main original important word as the form to use)

Other things to consider:
- proper nouns consisting of several words, eg. "City Park Apartments", "Salt Lake City" should stay together
- semantically related words, perhaps
- have a representation of what the article is about, so it can be tracked
- sentence still split incorrectly "yelp.com" becomes "yelp" and "com" (2 sentences), rework splitting

HTML Parsing and tokenizing:
- using cheerio to parse dom of extracted article
- v1: 
    -sentences tokenized using tokenizer library (natural node sentence tokenizer)
    -words extracted using tokenizer library (natural node tokenizer)
- v2: 
    -put sentences back together that were tokenized too aggresively (ie, "Mr.", URL's)
    -split into words based on capitalization (eg. keeping New York City as one word)
    -keeping words like I'm together
    -keeping words together like Dr. Cat
    -keeping words together that are hyphenated, like forty-seven


--June 2--
First, I calculated word frequencies for all of the words. The initial approach was very basic. Each different word was calculated independently, so words like "apartments" and "apartment" were calculated separately. Next, tf-idf values were calculated for each word, using a tf-idf library called natural node. The words with the highest scores were a good representation of topic words, but they definitely needed to be cleaned up because the same words, but with different endings, were at the top. So stemming of some sort was to be included.

Currently looking into the porter stemmer algorithm (still need to learn a little more about how it works exactly). Went through all the words, stemmed them, added to an object, and for each next word, stemmed it and added to a list of it's related words. 

Next need to keep certain words together, after tokenization, because things like city names, which consist of multiple words, can be considered as one single topic, so we don't want to remove the semantic meaning of those words when they're all together.

Noticed that when calculating tf-idf on a compound-word noun, its score was much higher that any single word alone. However, adding all the individual scores of a compound-word noun yield the same score! But there's an advantage of having the words together, because they can serve as a topic word, whereas when they're separated, their semantic meaning is lost.


--June 3--
Read the IBM art of tokenization article, helped a lot 
Considered detecting four different classes of words: contractions, like I'm, she's, ..., names that have a period in them, like Mr. Dog, words separated by hyphens, like forty-seven, and lastly, compound word nouns, like New York City

Spent some time doing them all in a single method, took too long to realize that doing that was a bad idea... too many things going on in a single method... also realized that detecting words like I'm and she's are of no use to me when generating a signature for the article... will only consider the last 3 cases, and will do them in separate methods, where different methods of tokenization may be appropriate..

On a broader spectrum, need to consider how a single article will be represented, what data structure, etc. What data needs to be stored, like timestamp, individual signature, topic words perhaps. Even broader, how to represent a story, eg. Brexit, and maybe what articles go under it, what the topic words are for all, progression of signatures, its own main signature (summary of all articles combined, to this day)

Orrrr... I should look into some of these topics: Named Entity Recognition, n-grams, tries. 
So maybe step 1 could be my manual approach, but then step 2 could be a more sophisticated approach